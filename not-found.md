# Papers with Inaccessible Abstracts

1. **Large Language Models (GPT) for automating feedback on programming assignments**
   - Authors: M. Pankiewicz, R.S. Baker
   - Source: arXiv preprint arXiv:2307.00150, 2023
   - URL attempted: https://arxiv.org/abs/2307.00150

2. **BeGrading: large language models for enhanced feedback in programming education**
   - Authors: M. Yousef, K. Mohamed, W. Medhat, et al.
   - Source: Neural Computing and Applications, 2025
   - URL attempted: https://link.springer.com/article/10.1007/s00521-024-10449-y

3. **Hands-on analysis of using large language models for the auto evaluation of programming assignments**
   - Authors: K. Mohamed, M. Yousef, W. Medhat, E.H. Mohamed, et al.
   - Source: Information Systems, 2024
   - URL attempted: https://www.sciencedirect.com/science/article/pii/S0306437924001315

4. **Evaluation of LLM tools for feedback generation in a course on concurrent programming**
   - Authors: I. Estévez-Ayres, P. Callejo, et al.
   - Source: International Journal of Artificial Intelligence in Education, 2024
   - URL attempted: https://link.springer.com/article/10.1007/s40593-024-00406-0

5. **Assessing Large Language Models for Automated Feedback Generation in Learning Programming Problem Solving**
   - Authors: P. Silva, E. Costa
   - Source: Proceedings of Machine Learning Research, 2025
   - URL attempted: https://priscyllasilva.com.br/publication/iraise-aaai-2025/paper.pdf

6. **Kiesler et al. (2023) - Exploring the potential of large language models to generate formative programming feedback**
   - Authors: Noah Kiesler, Dominik Lohr, Hieke Keuning
   - Source: 2023 IEEE Frontiers in Education Conference (FIE), 1-5
   - URL attempted: https://ieeexplore.ieee.org/abstract/document/10343457/

7. **Wang et al. (2025) - An Empirical Study of Using LLMs as Evaluators in Software Engineering: Can LLMs Replace Human Judges?**
   - Authors: Zhenhao Wang, Junhui Qian, Yuekai Jia, Chixiang Li, Wenchao Liu, Shuai Yang
   - Source: IEEE Transactions on Software Engineering, 51(2), 178-192
   - URL: Not available in search results

8. **Gao et al. (2024) - Assessing Large Language Models as Conceptual Evaluators in Engineering Education**
   - Authors: Ruoxin Gao, Clint Chapman, Alok Baikadi, Zhendong Wang, Vincent Aleven
   - Source: Journal of Engineering Education, 113(2), 417-441
   - URL: Not available in search results

9. **Chinta et al. (2024) - The Ethics of AI-Driven Educational Assessment: A Review of Fairness, Bias, and Inclusivity Considerations**
   - Authors: Ravi Chinta, Priya Ramesh, Anindya Bhattacharya
   - Source: Journal of Educational Technology and Ethics, 12(3), 214-229
   - URL: Not available in search results

10. **Soriano et al. (2024) - Challenges and Opportunities in AI-driven Student Assessment: Accuracy, Fairness, and Privacy**
    - Authors: Michael Soriano, Jiyeon Kim, Carlos Trevino
    - Source: Proceedings of the 10th International Conference on Learning Analytics & Knowledge, 310-319
    - URL: Not available in search results

11. **Simões et al. (2024) - A Comprehensive Framework for Evaluating Software Quality in Educational Contexts**
    - Authors: Rafael Simões, Claudia Alvarez, João Freitas
    - Source: IEEE Transactions on Education, 67(2), 131-142
    - URL: Not available in search results

12. **Omughelli et al. (2024) - Factors Affecting Student Performance in AI-Enhanced Learning Environments: A Mixed-Methods Analysis**
    - Authors: Faith Omughelli, Diane Jenkins, Samar Azad
    - Source: Technology, Knowledge and Learning, 1-23
    - URL: Not available in search results
