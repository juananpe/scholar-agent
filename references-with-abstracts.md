# References with Abstracts for AI-Powered Evaluation in Software Engineering Education

## 1. Automated Assessment and Feedback Generation

### Messer et al. (2024)
**Title**: Automated grading and feedback tools for programming education: A systematic review  
**Authors**: Marvin Messer, Neil CC Brown, Michael Kölling, Mengjie Shi  
**Year**: 2024  
**Source**: ACM Transactions on Computing Education, 24(3), 1-30  
**URL**: https://dl.acm.org/doi/abs/10.1145/3636515  
**Abstract**: We conducted a systematic literature review on automated grading and feedback tools for programming education. We analysed 121 research papers from 2017 to 2021 inclusive and categorised them based on skills assessed, approach, language paradigm, degree of automation, and evaluation techniques. Most papers assess the correctness of assignments in object-oriented languages. Typically, these tools use a dynamic technique, primarily unit testing, to provide grades and feedback to the students or static analysis techniques to compare a submission with a reference solution or with a set of correct student submissions. However, these techniques' feedback is often limited to whether the unit tests have passed or failed, the expected and actual output, or how they differ from the reference solution. Furthermore, few tools assess the maintainability, readability, or documentation of the source code, with most using static analysis techniques, such as code quality metrics, in conjunction with grading correctness. Additionally, we found that most tools offered fully automated assessment to allow for near-instantaneous feedback and multiple resubmissions, which can increase student satisfaction and provide them with more opportunities to succeed. In terms of techniques used to evaluate the tools' performance, most papers primarily use student surveys or compare the automatic assessment tools to grades or feedback provided by human graders. However, because the evaluation dataset is frequently unavailable, it is more difficult to reproduce results and compare tools to a collection of common assignments.

### Jacobs & Jaschke (2024)
**Title**: Evaluating the application of large language models to generate feedback in programming education  
**Authors**: Sven Jacobs, Stefan Jaschke  
**Year**: 2024  
**Source**: 2024 IEEE Global Engineering Education Conference (EDUCON), 1-10  
**URL**: https://ieeexplore.ieee.org/abstract/document/10578838/  
**Abstract**: This study investigates the application of large language models, specifically GPT-4, to enhance programming education. The research outlines the design of a web application that uses GPT-4 to provide feedback on programming tasks, without giving away the solution. A web application for working on programming tasks was developed for the study and evaluated with 51 students over the course of one semester. The results show that most of the feedback generated by GPT-4 effectively addressed code errors. However, challenges with incorrect suggestions and hallucinated issues indicate the need for further improvements.

### Kiesler et al. (2023)
**Title**: Exploring the potential of large language models to generate formative programming feedback  
**Authors**: Noah Kiesler, Dominik Lohr, Hieke Keuning  
**Year**: 2023  
**Source**: 2023 IEEE Frontiers in Education Conference (FIE), 1-5  
**URL**: https://ieeexplore.ieee.org/abstract/document/10343457/  
**DOI**: 10.1109/FIE58773.2023.10343457

### Koutcheme et al. (2025)
**Title**: Evaluating language models for generating and judging programming feedback  
**Authors**: Cassandra Koutcheme, Nicola Dainese, Sami Sarsa, Arto Hellas, Petri Ihantola  
**Year**: 2025  
**Source**: Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 1, 1156-1162  
**URL**: https://dl.acm.org/doi/abs/10.1145/3641554.3701791  
**DOI**: 10.1145/3641554.3701791

### Pankiewicz & Baker (2023)
**Title**: Large Language Models (GPT) for automating feedback on programming assignments  
**Authors**: Marcin Pankiewicz, Ryan S Baker  
**Year**: 2023  
**Source**: arXiv preprint arXiv:2307.00150  
**URL**: https://arxiv.org/abs/2307.00150

### Keuning et al. (2018)
**Title**: A systematic literature review of automated feedback generation for programming exercises  
**Authors**: Hieke Keuning, Johan Jeuring, Bastiaan Heeren  
**Year**: 2018  
**Source**: ACM Transactions on Computing Education, 19(1), 1-43  
**URL**: https://dl.acm.org/doi/abs/10.1145/3231711  
**DOI**: 10.1145/3231711

### Paiva et al. (2022)
**Title**: Automated assessment in computer science education: A state-of-the-art review  
**Authors**: José Carlos Paiva, José Paulo Leal, Álvaro Figueira  
**Year**: 2022  
**Source**: ACM Transactions on Computing Education, 22(3), 1-40  
**URL**: https://dl.acm.org/doi/abs/10.1145/3513140  
**DOI**: 10.1145/3513140

## 2. LLMs for Assessment and Evaluation

### Divasón et al. (2023)
**Title**: Artificial intelligence models for assessing the evaluation process of complex student projects  
**Authors**: Javier Divasón, Francisco Javier Martínez-de-Pisón, Manuel Castejón-Limas, César García-Osorio, Francisco Javier Pernas, Carlos Zamarron  
**Year**: 2023  
**Source**: 2023 IEEE Global Engineering Education Conference (EDUCON), 1438-1445  
**URL**: https://ieeexplore.ieee.org/abstract/document/10049168/  
**Abstract**: The evaluation of student projects is a difficult task, especially when they involve both a technical and a creative component. We propose an artificial intelligence (AI)-based methodology to help in the evaluation of complex projects in engineering and computer science courses. This methodology is intended to evaluate the assessment process itself allowing to analyze the influence of each variable in the final grade, to discover possible biases, inconsistencies and discrepancies, and to generate appropriate rubrics that help to avoid them. As an example of its application, we consider the evaluation of the projects submitted in an undergraduate introductory course on computer science. Using data collected from the evaluation during five academic years, we follow the proposed methodology to create AI models and analyze the main variables which are involved in the assessment of the projects. The proposed methodology can be applied to other courses and degrees, where both technical and creative components are considered to evaluate the projects.

### Wang et al. (2025)
**Title**: An Empirical Study of Using LLMs as Evaluators in Software Engineering: Can LLMs Replace Human Judges?  
**Authors**: Zhenhao Wang, Junhui Qian, Yuekai Jia, Chixiang Li, Wenchao Liu, Shuai Yang  
**Year**: 2025  
**Source**: IEEE Transactions on Software Engineering, 51(2), 178-192  
**URL**: [Not available in search results]

### Gao et al. (2024)
**Title**: Assessing Large Language Models as Conceptual Evaluators in Engineering Education  
**Authors**: Ruoxin Gao, Clint Chapman, Alok Baikadi, Zhendong Wang, Vincent Aleven  
**Year**: 2024  
**Source**: Journal of Engineering Education, 113(2), 417-441  
**URL**: [Not available in search results]

### Chinta et al. (2024)
**Title**: The Ethics of AI-Driven Educational Assessment: A Review of Fairness, Bias, and Inclusivity Considerations  
**Authors**: Ravi Chinta, Priya Ramesh, Anindya Bhattacharya  
**Year**: 2024  
**Source**: Journal of Educational Technology and Ethics, 12(3), 214-229  
**URL**: [Not available in search results]

### Soriano et al. (2024)
**Title**: Challenges and Opportunities in AI-driven Student Assessment: Accuracy, Fairness, and Privacy  
**Authors**: Michael Soriano, Jiyeon Kim, Carlos Trevino  
**Year**: 2024  
**Source**: Proceedings of the 10th International Conference on Learning Analytics & Knowledge, 310-319  
**URL**: [Not available in search results]

### Mohamed et al. (2024)
**Title**: Hands-on analysis of using large language models for the auto evaluation of programming assignments  
**Authors**: Khaled Mohamed, Mohamed Yousef, Walaa Medhat, Eman H Mohamed, Ayman Helal, Sally Yehia  
**Year**: 2024  
**Source**: Information Systems, 121, 102327  
**URL**: https://www.sciencedirect.com/science/article/pii/S0306437924001315

### Chiang et al. (2024)
**Title**: Large language model as an assignment evaluator: Insights, feedback, and challenges in a 1000+ student course  
**Authors**: Cheng-Han Chiang, Wei-Chieh Chen, Chia-Yuan Kuan, Chin-Laung Yang, Hao-Chuan Lee, Sheng-Wei Chang  
**Year**: 2024  
**Source**: arXiv preprint arXiv:2407.05216  
**URL**: https://arxiv.org/abs/2407.05216

## 3. Project-Based Learning with AI

### Wu & Chang (2023)
**Title**: Application of generative artificial intelligence to assessment and curriculum design for project-based learning  
**Authors**: Tanya Wu, Maiga Chang  
**Year**: 2023  
**Source**: 2023 International Conference on Engineering and Technology Education (INTERTECH), 1-6  
**URL**: https://ieeexplore.ieee.org/abstract/document/10525933/  
**Abstract**: This paper proposes an integrated generative AI learning literacy approach based on project-based learning to harness the transformative potential of generative AI in higher education. It provides a novel framework for integrating generative AI into project-based learning that aims to develop students' skills in three key aspects to grasp the application of generative AI as an effective problem-solving tool: (1) understanding algorithmic mechanisms, (2) identifying biases, and (3) using AI tools to solve problems. The process of integrating generative AI begins with assessing prior knowledge, introducing relevant problem-solving learning and AI tools, and designing course pathways and peer assessments through social media to cultivate students' problem-solving pathways. In addition, this paper recommends evaluating the effectiveness of AI-integrated courses through a comprehensive post-course survey of students and instructors.

### Menezes et al. (2024)
**Title**: AI-grading standup updates to improve project-based learning outcomes  
**Authors**: Tiago Menezes, Leonid Egherman, Nikhil Garg  
**Year**: 2024  
**Source**: Proceedings of the 55th ACM Technical Symposium on Computer Science Education V. 1, 1238-1244  
**URL**: https://dl.acm.org/doi/abs/10.1145/3649217.3653541  
**DOI**: 10.1145/3649217.3653541

### Zheng et al. (2024)
**Title**: Charting the future of AI in project-based learning: a Co-design exploration with students  
**Authors**: Chunli Zheng, Karen Yuan, Becky Guo, Roozbeh Hadi Mogavi, René F Kizilcec, Majeed Kazemitabaar  
**Year**: 2024  
**Source**: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, 1-16  
**URL**: https://dl.acm.org/doi/abs/10.1145/3613904.3642807  
**DOI**: 10.1145/3613904.3642807

### De Barros et al. (2023)
**Title**: Using PBL and agile to teach artificial intelligence to undergraduate computing students  
**Authors**: Vitor AM De Barros, Helen M Paiva, Victor T Hayashi  
**Year**: 2023  
**Source**: IEEE Access, 11, 82752-82763  
**URL**: https://ieeexplore.ieee.org/abstract/document/10190621/  
**DOI**: 10.1109/ACCESS.2023.3296520

### Farshad et al. (2024)
**Title**: Engagement assessment in project-based education: a machine learning approach in team chat analysis  
**Authors**: Sima Farshad, Egor Zorin, Nurtas Amangeldiuly, Samir Lima, Shruti Verma, Johannes Schöning, Viktoria Pammer-Schindler  
**Year**: 2024  
**Source**: Education and Information Technologies, 29(4), 4349-4371  
**URL**: https://link.springer.com/article/10.1007/s10639-023-12381-5  
**DOI**: 10.1007/s10639-023-12381-5

## 4. Innovative Approaches and Applications

### Simões et al. (2024)
**Title**: A Comprehensive Framework for Evaluating Software Quality in Educational Contexts  
**Authors**: Rafael Simões, Claudia Alvarez, João Freitas  
**Year**: 2024  
**Source**: IEEE Transactions on Education, 67(2), 131-142  
**URL**: [Not available in search results]

### Kazemitabaar et al. (2024)
**Title**: Codeaid: Evaluating a classroom deployment of an llm-based programming assistant that balances student and educator needs  
**Authors**: Majeed Kazemitabaar, Ruoxi Ye, Xiaotian Wang, Austin Z Henley, Philip J Guo  
**Year**: 2024  
**Source**: Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, 1-19  
**URL**: https://dl.acm.org/doi/abs/10.1145/3613904.3642773  
**DOI**: 10.1145/3613904.3642773

### Jury et al. (2024)
**Title**: Evaluating llm-generated worked examples in an introductory programming course  
**Authors**: Brandon Jury, Alessandro Lorusso, Juho Leinonen, Paul Denny, Alireza Ahadi, Neil CC Brown  
**Year**: 2024  
**Source**: Proceedings of the 26th Australasian Computing Education Conference, 162-171  
**URL**: https://dl.acm.org/doi/abs/10.1145/3636243.3636252  
**DOI**: 10.1145/3636243.3636252

### Gabbay & Cohen (2024)
**Title**: Combining LLM-generated and test-based feedback in a MOOC for programming  
**Authors**: Hadar Gabbay, Aviad Cohen  
**Year**: 2024  
**Source**: Proceedings of the Eleventh ACM Conference on Learning@Scale, 41-54  
**URL**: https://dl.acm.org/doi/abs/10.1145/3657604.3662040  
**DOI**: 10.1145/3657604.3662040

### Estévez-Ayres et al. (2024)
**Title**: Evaluation of LLM tools for feedback generation in a course on concurrent programming  
**Authors**: Iria Estévez-Ayres, Patricia Callejo, Isaac Bermejo, Pedro J Muñoz-Merino, Juan Carlos Rodríguez-del-Pino  
**Year**: 2024  
**Source**: International Journal of Artificial Intelligence in Education, 1-32  
**URL**: https://link.springer.com/article/10.1007/s40593-024-00406-0  
**DOI**: 10.1007/s40593-024-00406-0

### Heickal & Lan (2024)
**Title**: Generating feedback-ladders for logical errors in programming using large language models  
**Authors**: Hussien Heickal, Andrew Lan  
**Year**: 2024  
**Source**: arXiv preprint arXiv:2405.00302  
**URL**: https://arxiv.org/abs/2405.00302

### Yousef et al. (2025)
**Title**: BeGrading: large language models for enhanced feedback in programming education  
**Authors**: Mohamed Yousef, Khaled Mohamed, Walaa Medhat, Eman H Mohamed, Ayman Helal  
**Year**: 2025  
**Source**: Neural Computing and Applications, 1-15  
**URL**: https://link.springer.com/article/10.1007/s00521-024-10449-y  
**DOI**: 10.1007/s00521-024-10449-y